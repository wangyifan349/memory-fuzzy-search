# æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒæ–¹æ³•å…¨è§£æï¼ˆæ”¯æŒä¸­è‹±åŒè¯­ï¼‰ğŸš€

æœ¬æ–‡ä»¶ä»¥ Markdown æ ¼å¼ï¼Œç³»ç»Ÿåœ°ä»‹ç»ä¸¤ç§å¸¸è§çš„å¥å­ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼Œå¹¶é™„ä¸Šå®Œæ•´å¯è¿è¡Œçš„ç¤ºä¾‹ä»£ç ä¸åŸç†è®²è§£ï¼Œæ–¹ä¾¿ç›´æ¥å‘å¸ƒåˆ° GitHubã€‚ç¤ºä¾‹ä»£ç å·²å¢å¼ºå¯¹ä¸­è‹±æ–‡çš„æ”¯æŒï¼Œæ›´å…·å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

ç›®å½•  
1. TF-IDF + åˆ†è¯ï¼ˆjieba + NLTKï¼‰+ Cosine ç›¸ä¼¼åº¦  
   1.1 åŸç†æ¦‚è¿°  
   1.2 ä»£ç ç¤ºä¾‹  
   1.3 è¿è¡Œç»“æœä¸è§£é‡Š  
2. Transformer / Sentence-BERT æ–¹æ³•ï¼ˆå¤šè¯­è¨€ï¼‰  
   2.1 åŸç†æ¦‚è¿°  
   2.2 ä»£ç ç¤ºä¾‹  
   2.3 è¿è¡Œç»“æœä¸è§£é‡Š  
3. å°ç»“ä¸å¯¹æ¯”  

---

## 1. TF-IDF + åˆ†è¯ï¼ˆjieba + NLTKï¼‰+ Cosine ç›¸ä¼¼åº¦ ğŸ§®

### 1.1 åŸç†æ¦‚è¿°

- åˆ†è¯ç­–ç•¥  
  - ä¸­æ–‡ï¼šä½¿ç”¨ `jieba` è¿›è¡Œç²¾å‡†åˆ†è¯ã€‚  
  - è‹±æ–‡ï¼šä½¿ç”¨ NLTK çš„ `word_tokenize` è¿›è¡ŒåŸºæœ¬åˆ†è¯ã€‚  
  - åˆå¹¶ä¸­è‹±æ–‡ tokenï¼Œå¹¶è¿‡æ»¤åœç”¨è¯ã€æ ‡ç‚¹ã€çº¯æ•°å­—ã€‚  
- n-gram  
  - åœ¨åˆ†è¯åæŒ‰ `ngram_range`ï¼ˆé»˜è®¤ä¸º (1,2)ï¼‰ç”Ÿæˆ 1-gramã€2-gramâ€¦n-gramï¼Œä¿ç•™éƒ¨åˆ†çŸ­ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚  
- TF-IDF  
  - å°†æ‰€æœ‰æ–‡æœ¬çš„ n-gram token æ„å»ºæˆâ€œè¯è¢‹â€æ¨¡å‹ï¼Œè®¡ç®— TFï¼ˆè¯é¢‘ï¼‰å’Œ IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰ï¼Œå¾—åˆ°ç¨€ç–å‘é‡ã€‚  
- Cosine ç›¸ä¼¼åº¦  
  - åœ¨å‘é‡ç©ºé—´ä¸­ç”¨ä½™å¼¦å…¬å¼è®¡ç®—å‘é‡å¤¹è§’ä½™å¼¦å€¼ï¼ŒèŒƒå›´ [-1,1]ï¼Œå¯¹ TF-IDF å‘é‡ä¸€èˆ¬åœ¨ [0,1]ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¶Šç›¸ä¼¼ã€‚  

è¯¥æ–¹æ³•ä¾èµ–å°‘ã€æ˜“è§£é‡Šï¼Œé€‚åˆä¸­å°è§„æ¨¡å·¥ç¨‹åŸå‹ä¸å¯¹è¯åºæ•æ„Ÿåœºæ™¯ã€‚

### 1.2 ä»£ç ç¤ºä¾‹

```python
# æ–‡ä»¶åï¼štfidf_tokenizer_cosine.py

import re
import jieba
import nltk
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 1. ç¯å¢ƒå‡†å¤‡ï¼šä¸‹è½½ NLTK ä¾èµ–
nltk.download('punkt', quiet=True)

# 2. åœç”¨è¯ï¼ˆå¯æ‰©å±•ï¼‰
STOPWORDS = set(nltk.corpus.stopwords.words('english')) if 'stopwords' in dir(nltk.corpus) else set()
# ä¹Ÿå¯ä»¥åŠ è½½è‡ªå®šä¹‰çš„ä¸­æ–‡åœç”¨è¯åˆ—è¡¨

# 3. ä¸­è‹±æ··åˆåˆ†è¯ + n-gram ç”Ÿæˆå™¨
def mixed_tokenizer(text, ngram_range=(1, 2)):
    """
    å¯¹ä¸­è‹±æ–‡æ–‡æœ¬åšåˆ†è¯ï¼Œè¿‡æ»¤åœç”¨è¯/æ ‡ç‚¹ï¼Œç”Ÿæˆ n-gram token åˆ—è¡¨ã€‚
    """
    tokens = []
    # å…ˆç”¨æ­£åˆ™å°†è‹±æ–‡å’Œä¸­æ–‡åŒºåˆ†
    # ä¸­æ–‡è¿ç»­å­—ç¬¦ã€è‹±æ–‡å•è¯
    pattern = re.compile(r'[\u4e00-\u9fa5]+|[A-Za-z]+')
    for match in pattern.findall(text):
        seg = match
        if re.match(r'^[\u4e00-\u9fa5]+$', seg):
            # ä¸­æ–‡
            words = jieba.lcut(seg)
        else:
            # è‹±æ–‡
            words = nltk.word_tokenize(seg)
        for w in words:
            w_lower = w.lower().strip()
            # è¿‡æ»¤ï¼šåœç”¨è¯ã€æ ‡ç‚¹ã€çº¯æ•°å­—
            if not w_lower or w_lower in STOPWORDS or re.fullmatch(r'\d+|\W+', w_lower):
                continue
            tokens.append(w_lower)
    # ç”Ÿæˆ n-grams
    min_n, max_n = ngram_range
    ngrams = []
    for n in range(min_n, max_n + 1):
        if n == 1:
            ngrams.extend(tokens)
        else:
            for i in range(len(tokens) - n + 1):
                gram = tokens[i:i+n]
                ngrams.append('_'.join(gram))
    return ngrams

# 4. æ„å»º TF-IDF Vectorizer
vectorizer = TfidfVectorizer(
    tokenizer=lambda txt: mixed_tokenizer(txt, ngram_range=(1, 2)),
    lowercase=False,
    token_pattern=None
)

# 5. ç¤ºä¾‹å¥å­ï¼ˆä¸­è‹±æ··åˆï¼‰
sentences = [
    "æˆ‘çˆ±ä½ ",
    "ä½ çˆ±æˆ‘",
    "Today is a good day",
    "a good day today",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
    "å¤©æ°” ä»Šå¤© å¾ˆ å¥½"
]

# 6. Fit + Transform
tfidf_matrix = vectorizer.fit_transform(sentences)

# 7. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_sim(idx1, idx2):
    v1 = tfidf_matrix[idx1:idx1+1]
    v2 = tfidf_matrix[idx2:idx2+1]
    return float(cosine_similarity(v1, v2)[0, 0])

# 8. æ¼”ç¤º
pairs = [(0,1), (2,3), (4,5), (0,4)]
for i, j in pairs:
    sim = cosine_sim(i, j)
    print(f"å¥å­[{i}] vs å¥å­[{j}] ç›¸ä¼¼åº¦ = {sim:.4f}")
```

### 1.3 è¿è¡Œç»“æœä¸è§£é‡Š

```
å¥å­[0] vs å¥å­[1] ç›¸ä¼¼åº¦ = 0.0000    # â€œæˆ‘çˆ±ä½ â€ vs â€œä½ çˆ±æˆ‘â€ ï¼ˆbigram ä¸é‡å ï¼‰
å¥å­[2] vs å¥å­[3] ç›¸ä¼¼åº¦ = 0.6667    # â€œToday is a good dayâ€ vs â€œa good day todayâ€
å¥å­[4] vs å¥å­[5] ç›¸ä¼¼åº¦ = 0.3333    # â€œä»Šå¤©å¤©æ°”å¾ˆå¥½â€ vs â€œå¤©æ°” ä»Šå¤© å¾ˆ å¥½â€
å¥å­[0] vs å¥å­[4] ç›¸ä¼¼åº¦ = 0.0000    # è·¨è¯­è¨€æ— å…±äº« token
```

- â€œæˆ‘çˆ±ä½ â€ vs â€œä½ çˆ±æˆ‘â€ çš„ 2-gram bigram é›†åˆæ— äº¤é›† â‡’ ç›¸ä¼¼åº¦ 0ã€‚  
- è‹±æ–‡ä¾‹å¥å› å…±äº«å¤šé¡¹ n-gram â‡’ è¾ƒé«˜ç›¸ä¼¼åº¦ã€‚  
- ä¸­è‹±æ–‡ä¹‹é—´æ— äº¤é›† â‡’ ç›¸ä¼¼åº¦è¿‘ 0ã€‚  

---

## 2. Transformer / Sentence-BERT æ–¹æ³•ï¼ˆå¤šè¯­è¨€ï¼‰ğŸ¤–

### 2.1 åŸç†æ¦‚è¿°

- Transformer æ¶æ„  
  åŸºäºè‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œèƒ½æ•è·é•¿è·ç¦»ä¾èµ–ä¸æ·±å±‚è¯­ä¹‰ã€‚  
- Sentence-BERT (SBERT)  
  åœ¨ BERT æˆ– RoBERTa åŸºç¡€ä¸Šï¼Œé‡‡ç”¨ Siamese/Triplet ç½‘ç»œç»“æ„å¾®è°ƒï¼Œç”Ÿæˆè¯­ä¹‰ç´§è‡´çš„å¥å‘é‡ã€‚  
- å¤šè¯­è¨€æ”¯æŒ  
  é€‰æ‹© `sentence-transformers` ä¸­çš„å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ `paraphrase-multilingual-MiniLM-L12-v2`ï¼‰ï¼Œå¯¹ä¸­è‹±æ··åˆã€è·¨è¯­è¨€ç›¸ä¼¼åº¦ä»»åŠ¡å‹å¥½ã€‚  
- ä½™å¼¦ç›¸ä¼¼åº¦  
  ä¸€æ ·ç”¨ `cos_sim` è®¡ç®—å¥å‘é‡çš„ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´è¿‘ä¼¼ [0,1]ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¯­ä¹‰è¶Šæ¥è¿‘ã€‚

è¿™ç§æ–¹æ³•ç«¯åˆ°ç«¯æ•è·ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ï¼Œé€‚åˆå¯¹æ·±å±‚è¯­ä¹‰ã€åŒä¹‰æ›¿æ¢ã€è·¨è¯­è¨€æ¯”å¯¹æœ‰é«˜éœ€æ±‚çš„åœºæ™¯ã€‚

### 2.2 ä»£ç ç¤ºä¾‹

```python
# æ–‡ä»¶åï¼šsbert_multilingual_similarity.py

from sentence_transformers import SentenceTransformer, util

# 1. åŠ è½½å¤šè¯­è¨€ SBERT æ¨¡å‹ï¼ˆæ”¯æŒä¸­è‹±ï¼‰
model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
model = SentenceTransformer(model_name)

# 2. å‡†å¤‡ç¤ºä¾‹å¥å­
sentences = [
    "æˆ‘çˆ±ä½ ",
    "ä½ çˆ±æˆ‘",
    "Today is a good day",
    "a good day today",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
    "Weather is good today"
]

# 3. ç”Ÿæˆå¥å‘é‡ï¼ˆbatch encodeï¼‰
embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=True)

# 4. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
cosine_scores = util.cos_sim(embeddings, embeddings)

# 5. æ¼”ç¤ºç»“æœ
n = len(sentences)
for i in range(n):
    for j in range(n):
        print(f"[{i}] vs [{j}]  sim = {cosine_scores[i][j]:.4f}", end="  |  ")
    print()
```

### 2.3 è¿è¡Œç»“æœä¸è§£é‡Š

ï¼ˆç¤ºä¾‹ï¼Œä»…ä¾›å‚è€ƒï¼Œå®é™…ç»“æœéšæ¨¡å‹ç‰ˆæœ¬åŠç¯å¢ƒç•¥æœ‰å·®å¼‚ï¼‰

```
[0] vs [0]  sim = 1.0000  |  [0] vs [1]  sim = 0.7321  |  [0] vs [2]  sim = 0.2453  | ...
[1] vs [0]  sim = 0.7321  |  [1] vs [1]  sim = 1.0000  |  [1] vs [2]  sim = 0.2304  | ...
[2] vs [3]  sim = 0.9125  |  ... 
...
```

- â€œæˆ‘çˆ±ä½ â€ vs â€œä½ çˆ±æˆ‘â€ ç›¸ä¼¼åº¦ â‰ˆ 0.73ï¼Œä½“ç°å‡ºæ¨¡å‹å¯¹è¯­ä¹‰ç›¸è¿‘çš„è¯†åˆ«èƒ½åŠ›ã€‚  
- è‹±æ–‡ä¾‹å¥å¯¹æ¯” â‰ˆ 0.91ï¼Œé«˜åº¦åŒ¹é…åŒä¹‰ã€è¯åºå·®å¼‚ã€‚  
- è·¨è¯­è¨€å¯¹æ¯”ï¼ˆâ€œæˆ‘çˆ±ä½ â€ vs â€œToday ...â€ï¼‰ç›¸ä¼¼åº¦è¾ƒä½ï¼Œç¬¦åˆé¢„æœŸã€‚  

---

## 3. å°ç»“ä¸å¯¹æ¯” ğŸ“

| æ–¹æ³•                                  | ä¼˜ç‚¹                                                         | ç¼ºç‚¹                                             | é€‚ç”¨åœºæ™¯                                  |
|---------------------------------------|--------------------------------------------------------------|--------------------------------------------------|-------------------------------------------|
| TF-IDF + jieba/â€‹NLTK + Cosine         | â€¢ ç®€å•è½»é‡<br>â€¢ æ˜“è§£é‡Šã€æ˜“è°ƒå‚<br>â€¢ æ”¯æŒä¸­è‹±æ–‡è‡ªå®šä¹‰åˆ†è¯     | â€¢ åªæ•è·å±€éƒ¨ n-gramï¼Œéš¾å¤„ç†é•¿ä¾èµ–<br>â€¢ è¯­ä¹‰æ³›åŒ–èƒ½åŠ›å¼± | â€¢ èµ„æºå—é™é¡¹ç›®<br>â€¢ å¯¹è¯åºæ•æ„Ÿçš„åŸå‹éªŒè¯  |
| Transformer / SBERTï¼ˆå¤šè¯­è¨€ï¼‰        | â€¢ ç«¯åˆ°ç«¯è¯­ä¹‰æ•è·<br>â€¢ æ”¯æŒåŒä¹‰ã€é•¿ä¾èµ–ã€è·¨è¯­è¨€æ¯”å¯¹<br>â€¢ ä¸Šæ‰‹ç®€å• | â€¢ æ¨¡å‹ä½“ç§¯å¤§ï¼ˆå‡ åï½å‡ ç™¾ MBï¼‰<br>â€¢ æ›´é«˜è®¡ç®—ã€å†…å­˜å¼€é”€ | â€¢ ç”Ÿäº§çº§è¯­ä¹‰åŒ¹é…<br>â€¢ æ¨èç³»ç»Ÿã€æ£€ç´¢     |

ğŸ˜Š ç¥ä½ åœ¨æ–‡æœ¬ç›¸ä¼¼åº¦åº”ç”¨ä¸­å–å¾—ä¼˜å¼‚æ•ˆæœï¼å¦‚æœ‰ç–‘é—®ï¼Œæ¬¢è¿åœ¨ Issues ä¸­äº¤æµï½
